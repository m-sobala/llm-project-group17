{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 19,
      "id": "b999d540",
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch\n",
        "from transformers import MarianMTModel, MarianTokenizer, AutoModelForSeq2SeqLM, AutoTokenizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "577180ac",
      "metadata": {},
      "outputs": [],
      "source": [
        "dataset_name = \"Iker/Document-Translation-en-es\"\n",
        "\n",
        "summary_model_name = \"t5-small\"\n",
        "translation_model_name = \"Helsinki-NLP/opus-mt-en-es\"\n",
        "revision = \"main\"\n",
        "\n",
        "device = 0 if torch.cuda.is_available() else -1"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "dbe381c7",
      "metadata": {},
      "source": [
        "## Base summariser"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "id": "1e05c503",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "the Hugging Face Transformers library provides state-of-the-art architectures for natural language understanding (NLU) and natural language generation (NLG). these architectures include BERT, GPT, GPT-2, and BART.\n"
          ]
        }
      ],
      "source": [
        "model = AutoModelForSeq2SeqLM.from_pretrained(summary_model_name)\n",
        "tokenizer = AutoTokenizer.from_pretrained(summary_model_name)\n",
        "text = \"\"\"\n",
        "The Hugging Face Transformers library provides state-of-the-art general-purpose architectures\n",
        "for natural language understanding (NLU) and natural language generation (NLG). These architectures\n",
        "include BERT, GPT, GPT-2, BART, and T5, which can be applied to text classification, summarization, translation, and more.\n",
        "\"\"\"\n",
        "\n",
        "inputs = tokenizer.encode(\"summarize: \" + text, return_tensors=\"pt\", max_length=1024, truncation=True)\n",
        "summary_ids = model.generate(inputs, max_length=150, min_length=40, length_penalty=2.0, num_beams=4, early_stopping=True)\n",
        "\n",
        "summary = tokenizer.decode(summary_ids[0], skip_special_tokens=True)\n",
        "\n",
        "print(summary)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6b79eec0",
      "metadata": {},
      "source": [
        "## Baseline translator"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "id": "c805fb9a",
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/Users/marek/dev/llm_venv/lib/python3.11/site-packages/transformers/models/marian/tokenization_marian.py:175: UserWarning: Recommended: pip install sacremoses.\n",
            "  warnings.warn(\"Recommended: pip install sacremoses.\")\n",
            "/Users/marek/dev/llm_venv/lib/python3.11/site-packages/transformers/tokenization_utils_base.py:1617: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be deprecated in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Translated Text (Spanish): Vamos a aprender IA usando Python\n"
          ]
        }
      ],
      "source": [
        "tokenizer = MarianTokenizer.from_pretrained(translation_model_name)\n",
        "model = MarianMTModel.from_pretrained(translation_model_name)\n",
        "english_text = \"Let's learn AI using Python\"\n",
        "input_ids = tokenizer.encode(english_text, return_tensors=\"pt\")\n",
        "translated_ids = model.generate(input_ids)\n",
        "\n",
        "translated_text = tokenizer.decode(translated_ids[0], skip_special_tokens=True)\n",
        "\n",
        "print(\"Translated Text (Spanish):\", translated_text)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "01b50f87",
      "metadata": {},
      "source": [
        "## Load dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "id": "0e617b56",
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Generating train split: 100%|██████████| 10533/10533 [00:00<00:00, 119201.57 examples/s]\n"
          ]
        },
        {
          "ename": "KeyError",
          "evalue": "'test'",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[22], line 5\u001b[0m\n\u001b[1;32m      3\u001b[0m dataset \u001b[38;5;241m=\u001b[39m load_dataset(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIker/Document-Translation-en-es\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      4\u001b[0m train_dataset \u001b[38;5;241m=\u001b[39m dataset[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mshuffle(seed\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m42\u001b[39m)\u001b[38;5;241m.\u001b[39mselect(\u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m100\u001b[39m))  \u001b[38;5;66;03m# Using a subset for quick fine-tuning\u001b[39;00m\n\u001b[0;32m----> 5\u001b[0m test_dataset \u001b[38;5;241m=\u001b[39m \u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtest\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241m.\u001b[39mshuffle(seed\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m42\u001b[39m)\u001b[38;5;241m.\u001b[39mselect(\u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m100\u001b[39m))\n",
            "File \u001b[0;32m~/dev/llm_venv/lib/python3.11/site-packages/datasets/dataset_dict.py:72\u001b[0m, in \u001b[0;36mDatasetDict.__getitem__\u001b[0;34m(self, k)\u001b[0m\n\u001b[1;32m     70\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__getitem__\u001b[39m(\u001b[38;5;28mself\u001b[39m, k) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Dataset:\n\u001b[1;32m     71\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(k, (\u001b[38;5;28mstr\u001b[39m, NamedSplit)) \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m---> 72\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__getitem__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mk\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     73\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     74\u001b[0m         available_suggested_splits \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m     75\u001b[0m             split \u001b[38;5;28;01mfor\u001b[39;00m split \u001b[38;5;129;01min\u001b[39;00m (Split\u001b[38;5;241m.\u001b[39mTRAIN, Split\u001b[38;5;241m.\u001b[39mTEST, Split\u001b[38;5;241m.\u001b[39mVALIDATION) \u001b[38;5;28;01mif\u001b[39;00m split \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n\u001b[1;32m     76\u001b[0m         ]\n",
            "\u001b[0;31mKeyError\u001b[0m: 'test'"
          ]
        }
      ],
      "source": [
        "from datasets import load_dataset\n",
        "\n",
        "dataset = load_dataset(dataset_name)\n",
        "train_dataset = dataset[\"train\"].shuffle(seed=42).select(range(100))  # Using a subset for quick fine-tuning\n",
        "test_dataset = dataset[\"test\"].shuffle(seed=42).select(range(100))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "121f8bab",
      "metadata": {},
      "source": [
        "## Load tokenizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6ccfb7fa",
      "metadata": {},
      "outputs": [],
      "source": [
        "from transformers import AutoTokenizer\n",
        "\n",
        "model_name = \"distilbert-base-uncased\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "\n",
        "def tokenize_function(examples):\n",
        "    # print(examples[\"text\"][0])\n",
        "    return tokenizer(examples[\"text\"], padding=\"max_length\", truncation=True)\n",
        "\n",
        "tokenized_train = train_dataset.map(tokenize_function, batched=True)\n",
        "tokenized_test = test_dataset.map(tokenize_function, batched=True)\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
