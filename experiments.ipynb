{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "b999d540",
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/Users/marek/dev/llm_venv/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
            "  from .autonotebook import tqdm as notebook_tqdm\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "from transformers import MarianMTModel, MarianTokenizer, AutoModelForSeq2SeqLM, AutoTokenizer, AutoModel\n",
        "from datasets import load_dataset\n",
        "import evaluate\n",
        "from context import summarize_text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "577180ac",
      "metadata": {},
      "outputs": [],
      "source": [
        "dataset_name = \"Iker/Document-Translation-en-es\"\n",
        "\n",
        "summary_model_name = \"google-t5/t5-small\"\n",
        "translation_model_name = \"Helsinki-NLP/opus-mt-en-es\"\n",
        "revision = \"main\"\n",
        "\n",
        "tokenizer_model_name = \"distilbert-base-uncased\"\n",
        "model_name = \"Helsinki-NLP/opus-mt-en-es\"\n",
        "\n",
        "device = 0 if torch.cuda.is_available() else -1"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fd7638fb",
      "metadata": {},
      "source": [
        "# Get Context"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "67f1e8b7",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'document.txt. document.txt. document.txt.'"
            ]
          },
          "execution_count": 3,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "text_file = 'document.txt'\n",
        "\n",
        "summarize_text(text_file, max_length=35)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "dbe381c7",
      "metadata": {},
      "source": [
        "## Base summariser"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "1e05c503",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "the Hugging Face Transformers library provides state-of-the-art architectures for natural language understanding (NLU) and natural language generation (NLG). these architectures include BERT, GPT, GPT-2, and BART.\n"
          ]
        }
      ],
      "source": [
        "model = AutoModelForSeq2SeqLM.from_pretrained(summary_model_name)\n",
        "tokenizer = AutoTokenizer.from_pretrained(summary_model_name)\n",
        "text = \"\"\"\n",
        "The Hugging Face Transformers library provides state-of-the-art general-purpose architectures\n",
        "for natural language understanding (NLU) and natural language generation (NLG). These architectures\n",
        "include BERT, GPT, GPT-2, BART, and T5, which can be applied to text classification, summarization, translation, and more.\n",
        "\"\"\"\n",
        "\n",
        "inputs = tokenizer.encode(\"summarize: \" + text, return_tensors=\"pt\", max_length=1024, truncation=True)\n",
        "summary_ids = model.generate(inputs, max_length=100, min_length=40, length_penalty=2.0, num_beams=4, early_stopping=True)\n",
        "\n",
        "summary = tokenizer.decode(summary_ids[0], skip_special_tokens=True)\n",
        "\n",
        "print(summary)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "e2cfbbe1",
      "metadata": {},
      "outputs": [],
      "source": [
        "text = 'Translate the following text from English to Spanish:  FIFA introduced anti-doping controls in 1966, but it wasn\\'t until 1974 that the first case occurred in a World Cup, and since then, there have only been three other positive cases. The first one was Haitian player Ernst Jean-Joseph, during the 1974 World Cup in Germany. The red-haired mixed-race midfielder tested positive for ephedrine - he claimed he had taken some asthma pills - after the match against Poland, but he was not sanctioned by FIFA. The punishment was imposed by dictator Jean Claude Duvalier, who two days later demanded his return to the Caribbean country: the Haitian police forcibly removed him from the hotel in front of the international press, put him into a car, and took him to the airport, back to his country to explain himself. Four years later, Scottish player Willy Johnstone tested positive for fencamfamine - a fatigue recoverer - after the match against Peru during the 1978 World Cup in Argentina. The skillful winger didn\\'t have to undergo the initial test, but Archie Gemmill, who was supposed to do it, was severely dehydrated. Johnstone tested positive and his excuses (\"I couldn\\'t have doped because I played the worst game of my international career,\" he said) were not enough for the Scottish federation, who sent him back to the United Kingdom. A Spaniard, Barcelona\\'s Ramón María Calderé, was the protagonist of the third case in World Cup history, testing positive for ephedrine during the 1986 World Cup in Mexico. A syrup he took with medical prescription from FIFA was the cause, so once the player\\'s innocence was proven, the sanction was reduced to a one-game suspension and a fine of 25,000 Swiss francs (around 9,000 current euros) for the Spanish Federation, for not administering it within the 72 hours before the match against Northern Ireland, as required. But if there is one anti-doping control that is well remembered, it is that of Diego Armando Maradona, who tested positive for five substances derived from ephedrine, after the match against Nigeria during the 1994 World Cup in the United States. Expelled from the World Cup and suspended for 15 months, that was the last game for the \"Pelusa\" with the Albiceleste.', 'Translate the following text from English to Spanish:  The Valencian Institute of Modern Art (IVAM) will exhibit works from today until February 23 by authors from its collection together with those of young Valencian artists, in a show called \"Sustratos\" (Substrates). Its title refers to the interest in linking the overlap of different artistic stages that take place in the city of Valencia, as explained in a statement by the museum, as well as to the place where this exhibition takes place. Its location is the Muralla room, from where \"the IVAM links its own modernity with the tradition of its host city,\" as it preserves the ancient fortress.\\n\\nThe exhibition is multidisciplinary and consists of productions by 20 artists under 35 years old who will interact with works by artists who, due to their recognized trajectory, are already present in the IVAM collection. Among the established artists are Eduardo Arroyo, Carmen Calvo, Miquel Navarro, Alberto Corazón, Sanleón, and Teresa Cháfer. Among the emerging artists, we can mention Señor Cifrián, Andrea Gussi, Paola Ruiz Moltó, or Marcos Juncal. In total, 42 creators.\\n\\nThey claim that \"Sustratos\" carries out a \"new relationship of works, gathered under the simple principle of matching some with others, in encounters that sometimes are obvious and sometimes surprising.\" \"When I delve into the exhibition,\" points out the curator of the show, Nilo Casares, \"I do it based on my circumstance as a resident in Valencia who is not from here, a status that allows me to pay attention to the plurality of origins that all artistic manifestations have, but also the artists themselves.\"\\n\\n\"In Sustratos,\" the curator adds, \"the challenge is to bring to the Muralla room the possible stories contained in Art History, the IVAM collection, and its development through the latest artistic productions linked to the Valencian Community, which without the IVAM would be different in the artistic field.\"', 'Translate the following text from English to Spanish:  There is still one matchday left for the end of the Second Division League, and except for a spot for promotion and the specific order for the playoffs, everything is decided. If a few days ago it was Unzue, coach of Numancia, who said he would change teams next season, today three other coaches have confirmed that they will not continue in the same benches next year. They are Javi López from Xerez, Onésimo Sánchez from Huesca, and Lucas Alcaráz from Córdoba, all for different reasons and with the season\\'s goals accomplished. One of the most surprising cases is that of Javi López, whose team is the only one of the three still playing for something. Xerez has reached the final matchday with possibilities of getting into the fight for promotion and taking the seventh place from Valladolid, something that will happen if they beat Elche and Valladolid lose to Alcorcón. López stated in a press conference that he informed the club of his decision two months ago and has complained that since then the atmosphere has been \"weird.\" \\n\\nOnésimo\\'s case was also unexpected; he announced it after securing Huesca\\'s safety by breaking the points record in the Second Division. \"It\\'s a very deliberate decision. I think a cycle has ended, and both parties should be happy with what we have achieved,\" commented the coach; \"everything seemed to point towards another year, but in the end, I wasn\\'t clear about it.\" In their final match, Huesca will visit a struggling Albacete.\\n\\nLucas Alcaraz has explained the reasons for his departure more thoroughly. The Córdoba coach has justified that the club, currently in insolvency proceedings, will soon change ownership, and his bond was with the current owner. \"I think, from my side, the commitment to bring the ship safely to port has been fulfilled, and given the change in circumstances, I wanted to announce my goodbye,\" he stated to the media. Córdoba had an irregular season; halfway through, it seemed they would be fighting at the top, but they had to wait until the last matchdays to secure their stay in the league. On Saturday, they face Girona, who announced today that they are negotiating to renew their coach, Raúl Agné. You can follow EL PAÍS Sports on Facebook, Twitter, or subscribe to the Newsletter here.', 'Translate the following text from English to Spanish:  Radio Valencia Cadena SER has gathered in a debate the five spokespeople of the municipal groups to analyze how the first year of the new local government has been. The opposition has provided a negative assessment, accusing the tripartite of not knowing how to manage and of dividing the citizens. The local government, on the other hand, has emphasized the achievements obtained in this first year of government and has responded to the accusations of the opposition, specifically the PP, recalling the delicate political situation of its councillors. Representing the PP, Eusebio Monzó has participated, pointing out that some of the new government\\'s achievements have been possible thanks to the inheritance left by the PP, with a surplus in the municipal accounts. Monzó has criticized the local government for supporting the dismantling of the single school district, for \"ruining\" the school vouchers, and for raising property taxes. He has reproached the local government for the partisan use of the balcony. Monzó, who has apologized for corruption, has demanded respect for the traditions and identity of the Valencians. Meanwhile, Fernando Giner from Ciudadanos has emphasized the missed opportunities in this year. He cites the strategic tourism plan, which will not be approved until after the summer, and the impatience growing in El Cabanyal. He also warns about the legal uncertainty caused by the reversal of tourist influx zones. Giner accuses the local government of not being united, wasting time on banners, prioritizing populism, and operating, he said, \"blinded by Ribó\\'s ideology.\" The local government presents a positive balance. Jordi Peris for València en Comú clarifies that there is indeed dialogue within the government, and their way of working is being imitated nationally, hence the term \"Valencian-style government\". Among the challenges are building more social housing and betting on innovation. He highlights the social shift in municipal policies, with social clauses in contracts to support citizens\\' rights, and new forms of collaboration with neighbors. Peris acknowledges that changes are slow, but they are happening. Joan Calabuig from PSPV has defended the property tax increase because \"those who have more were not the ones paying the most.\" He states that, despite the criticisms, the city is functioning and that suppliers are being paid within 14 days. Regarding the freedom of commercial hours and school choice demanded by the PP, Calabuig reminds that \"it was the PP that benefited from CIEGSA\" and that it arbitrarily created four tourist influx zones. Regarding future challenges, he says generating more employment should be the priority. On behalf of Compromís, Pere Fuset has stated that their priority has been the rescue of people and that Valencia is an anti-eviction city thanks to the local government. He criticized the fact that the city\\'s image has been tarnished by corruption and that \"the PP uses faith and festivities as elements of tension\". Regarding future challenges, he calls for the underground tunnel and highlights mobility as a change towards a friendlier city. He requests the collaboration of all Valencians. The debate was moderated by the Chief Editor of Radio Valencia, Julián Giménez, and was recorded in the main studio of TAU, the Audiovisual Workshop of the Universitat de València.'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "c20cc124",
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/Users/marek/dev/llm_venv/lib/python3.11/site-packages/transformers/tokenization_utils_base.py:1617: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be deprecated in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Token indices sequence length is longer than the specified maximum sequence length for this model (773 > 512). Running this sequence through the model will result in indexing errors\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "La primera vez era el primer caso de la enfermedad de la enfermedad de la UEFA, pero no era a pasar en 1966, pero no hay tres casos positivas. El primero era el jugador de la enfermedad en el UEFA, durante el ao 1974, y en el pasado de 1974, y en el pasado de 1974, hay sólo tres casos positivas. El primero era el jugador de la enfermedad mixed-race enfermedad en el UEFA, durante el jugador de la UEFA, durante el jugador de la UEFA, durante el jugador de la UEFA, durante el jugador de la UEFA, durante el jugador de la UEFA, durante el jugador de la UEFA, durante el jugador de la UEFA, durante el jugador de la UEFA, durante el jugador de la UEFA, durante el jugador de la UEFA, durante el jugador de la UEFA, durante el jugador de la UEFA, durante el jugador de la UEFA, durante el jugador de la UEFA, durante el jugador de la UEFA, durante el jugador de la UEFA, durante el jugador de la UEFA, durante el jugador de la UEFA, durante el jugador de la UEFA, durante el ju\n"
          ]
        }
      ],
      "source": [
        "from transformers import AutoModelForSeq2SeqLM, AutoTokenizer\n",
        "\n",
        "# Choose the specific Falcon-Instruct model. For example, \"tiiuae/falcon-7b-instruct\"\n",
        "# model_name = \"EleutherAI/gpt-neo-1.3B\"\n",
        "model_name = \"google/flan-t5-small\"\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "model = AutoModelForSeq2SeqLM.from_pretrained(model_name)\n",
        "\n",
        "inputs = tokenizer(text, return_tensors=\"pt\", padding=True)\n",
        "output = model.generate(**inputs, max_length=500)\n",
        "\n",
        "generated_text = tokenizer.decode(output[0], skip_special_tokens=True)\n",
        "print(generated_text)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6b79eec0",
      "metadata": {},
      "source": [
        "## Baseline translator"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "c805fb9a",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Translated Text (Spanish): ¿De qué color es la hrass? Respuesta en inglés\n"
          ]
        }
      ],
      "source": [
        "tokenizer = MarianTokenizer.from_pretrained(translation_model_name)\n",
        "model = MarianMTModel.from_pretrained(translation_model_name)\n",
        "english_text = \"What color is the hrass? Answer in english\"\n",
        "input_ids = tokenizer.encode(english_text, return_tensors=\"pt\")\n",
        "translated_ids = model.generate(input_ids)\n",
        "\n",
        "translated_text = tokenizer.decode(translated_ids[0], skip_special_tokens=True)\n",
        "\n",
        "print(\"Translated Text (Spanish):\", translated_text)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "01b50f87",
      "metadata": {},
      "source": [
        "## Load dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "0e617b56",
      "metadata": {},
      "outputs": [],
      "source": [
        "dataset = load_dataset(dataset_name)\n",
        "split_dataset = dataset[\"train\"].train_test_split(test_size=0.1, seed=42)\n",
        "train_dataset = split_dataset[\"train\"].shuffle(seed=42).select(range(100))\n",
        "test_dataset = split_dataset[\"test\"].shuffle(seed=42).select(range(100))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b4f22714",
      "metadata": {},
      "source": [
        "## x/y split"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "ba298518",
      "metadata": {},
      "outputs": [],
      "source": [
        "train_x_data = train_dataset[\"en\"]\n",
        "train_y_data = train_dataset[\"es\"]\n",
        "test_x_data = test_dataset[\"en\"]\n",
        "test_y_data = test_dataset[\"es\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "72390eb2",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "3100"
            ]
          },
          "execution_count": 11,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "len(train_x_data[0])"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8fe1f3c0",
      "metadata": {},
      "source": [
        "# Load Pre-Trained Models"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "id": "4b8f5def",
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "You are using a model of type t5 to instantiate a model of type marian. This is not supported for all configurations of models and can yield errors.\n"
          ]
        },
        {
          "ename": "ValueError",
          "evalue": "The state dictionary of the model you are trying to load is corrupted. Are you sure it was properly saved?",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[13], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mMarianMTModel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_name\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      2\u001b[0m bleu \u001b[38;5;241m=\u001b[39m evaluate\u001b[38;5;241m.\u001b[39mload(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msacrebleu\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      3\u001b[0m tokenizer \u001b[38;5;241m=\u001b[39m AutoTokenizer\u001b[38;5;241m.\u001b[39mfrom_pretrained(tokenizer_model_name)\n",
            "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\transformers\\modeling_utils.py:3960\u001b[0m, in \u001b[0;36mPreTrainedModel.from_pretrained\u001b[1;34m(cls, pretrained_model_name_or_path, config, cache_dir, ignore_mismatched_sizes, force_download, local_files_only, token, revision, use_safetensors, *model_args, **kwargs)\u001b[0m\n\u001b[0;32m   3950\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m dtype_orig \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   3951\u001b[0m         torch\u001b[38;5;241m.\u001b[39mset_default_dtype(dtype_orig)\n\u001b[0;32m   3953\u001b[0m     (\n\u001b[0;32m   3954\u001b[0m         model,\n\u001b[0;32m   3955\u001b[0m         missing_keys,\n\u001b[0;32m   3956\u001b[0m         unexpected_keys,\n\u001b[0;32m   3957\u001b[0m         mismatched_keys,\n\u001b[0;32m   3958\u001b[0m         offload_index,\n\u001b[0;32m   3959\u001b[0m         error_msgs,\n\u001b[1;32m-> 3960\u001b[0m     ) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mcls\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_load_pretrained_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   3961\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3962\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstate_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3963\u001b[0m \u001b[43m        \u001b[49m\u001b[43mloaded_state_dict_keys\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# XXX: rename?\u001b[39;49;00m\n\u001b[0;32m   3964\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresolved_archive_file\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3965\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3966\u001b[0m \u001b[43m        \u001b[49m\u001b[43mignore_mismatched_sizes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_mismatched_sizes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3967\u001b[0m \u001b[43m        \u001b[49m\u001b[43msharded_metadata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msharded_metadata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3968\u001b[0m \u001b[43m        \u001b[49m\u001b[43m_fast_init\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m_fast_init\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3969\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlow_cpu_mem_usage\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlow_cpu_mem_usage\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3970\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdevice_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice_map\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3971\u001b[0m \u001b[43m        \u001b[49m\u001b[43moffload_folder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moffload_folder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3972\u001b[0m \u001b[43m        \u001b[49m\u001b[43moffload_state_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moffload_state_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3973\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtorch_dtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3974\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhf_quantizer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhf_quantizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3975\u001b[0m \u001b[43m        \u001b[49m\u001b[43mkeep_in_fp32_modules\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkeep_in_fp32_modules\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3976\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgguf_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgguf_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3977\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   3979\u001b[0m \u001b[38;5;66;03m# make sure token embedding weights are still tied if needed\u001b[39;00m\n\u001b[0;32m   3980\u001b[0m model\u001b[38;5;241m.\u001b[39mtie_weights()\n",
            "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\transformers\\modeling_utils.py:4273\u001b[0m, in \u001b[0;36mPreTrainedModel._load_pretrained_model\u001b[1;34m(cls, model, state_dict, loaded_keys, resolved_archive_file, pretrained_model_name_or_path, ignore_mismatched_sizes, sharded_metadata, _fast_init, low_cpu_mem_usage, device_map, offload_folder, offload_state_dict, dtype, hf_quantizer, keep_in_fp32_modules, gguf_path)\u001b[0m\n\u001b[0;32m   4271\u001b[0m base_model_expected_keys \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(model_to_load\u001b[38;5;241m.\u001b[39mstate_dict()\u001b[38;5;241m.\u001b[39mkeys())\n\u001b[0;32m   4272\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28many\u001b[39m(key \u001b[38;5;129;01min\u001b[39;00m expected_keys_not_prefixed \u001b[38;5;129;01mand\u001b[39;00m key \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m base_model_expected_keys \u001b[38;5;28;01mfor\u001b[39;00m key \u001b[38;5;129;01min\u001b[39;00m loaded_keys):\n\u001b[1;32m-> 4273\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m   4274\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe state dictionary of the model you are trying to load is corrupted. Are you sure it was \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   4275\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mproperly saved?\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   4276\u001b[0m     )\n\u001b[0;32m   4277\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m device_map \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   4278\u001b[0m     device_map \u001b[38;5;241m=\u001b[39m {k\u001b[38;5;241m.\u001b[39mreplace(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39mbase_model_prefix\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m): v \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m device_map\u001b[38;5;241m.\u001b[39mitems()}\n",
            "\u001b[1;31mValueError\u001b[0m: The state dictionary of the model you are trying to load is corrupted. Are you sure it was properly saved?"
          ]
        }
      ],
      "source": [
        "model = MarianMTModel.from_pretrained(model_name)\n",
        "bleu = evaluate.load(\"sacrebleu\")\n",
        "tokenizer = AutoTokenizer.from_pretrained(tokenizer_model_name)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "121f8bab",
      "metadata": {},
      "source": [
        "## Load tokenizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "id": "6ccfb7fa",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "bcce574dd48149e883d6f923bc27f869",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Map:   0%|          | 0/100 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "9abb6f82b9c7442888ac9f48262f71aa",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Map:   0%|          | 0/100 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# I used the original text (english) to tokenize the data, seemed logical to me, but idk\n",
        "def tokenize_function(examples):\n",
        "    return tokenizer(examples[\"en\"], padding=\"max_length\", truncation=True)\n",
        "\n",
        "train_x_data = train_dataset.map(tokenize_function, batched=True)\n",
        "test_x_data = test_dataset.map(tokenize_function, batched=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5b815002",
      "metadata": {},
      "source": [
        "# Evaluate Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "id": "a290cadf",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "translating the tokens..\n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[15], line 4\u001b[0m\n\u001b[0;32m      2\u001b[0m input_ids \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor(test_x_data[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minput_ids\u001b[39m\u001b[38;5;124m\"\u001b[39m])\u001b[38;5;241m.\u001b[39mto(model\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtranslating the tokens..\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m----> 4\u001b[0m translated_tokens \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_ids\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      5\u001b[0m predicted_translations \u001b[38;5;241m=\u001b[39m [tokenizer\u001b[38;5;241m.\u001b[39mdecode(t, skip_special_tokens\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m) \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m translated_tokens]\n\u001b[0;32m      7\u001b[0m references \u001b[38;5;241m=\u001b[39m [[ref] \u001b[38;5;28;01mfor\u001b[39;00m ref \u001b[38;5;129;01min\u001b[39;00m test_y_data] \u001b[38;5;66;03m#The bleu score needs a list of lists\u001b[39;00m\n",
            "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\torch\\utils\\_contextlib.py:116\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    113\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[0;32m    114\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m    115\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[1;32m--> 116\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\transformers\\generation\\utils.py:2063\u001b[0m, in \u001b[0;36mGenerationMixin.generate\u001b[1;34m(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, assistant_model, streamer, negative_prompt_ids, negative_prompt_attention_mask, **kwargs)\u001b[0m\n\u001b[0;32m   2055\u001b[0m     input_ids, model_kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_expand_inputs_for_generation(\n\u001b[0;32m   2056\u001b[0m         input_ids\u001b[38;5;241m=\u001b[39minput_ids,\n\u001b[0;32m   2057\u001b[0m         expand_size\u001b[38;5;241m=\u001b[39mgeneration_config\u001b[38;5;241m.\u001b[39mnum_beams,\n\u001b[0;32m   2058\u001b[0m         is_encoder_decoder\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mis_encoder_decoder,\n\u001b[0;32m   2059\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmodel_kwargs,\n\u001b[0;32m   2060\u001b[0m     )\n\u001b[0;32m   2062\u001b[0m     \u001b[38;5;66;03m# 14. run beam sample\u001b[39;00m\n\u001b[1;32m-> 2063\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_beam_search\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   2064\u001b[0m \u001b[43m        \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2065\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbeam_scorer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2066\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlogits_processor\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprepared_logits_processor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2067\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlogits_warper\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprepared_logits_warper\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2068\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstopping_criteria\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprepared_stopping_criteria\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2069\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgeneration_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgeneration_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2070\u001b[0m \u001b[43m        \u001b[49m\u001b[43msynced_gpus\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msynced_gpus\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2071\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2072\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2074\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m generation_mode \u001b[38;5;241m==\u001b[39m GenerationMode\u001b[38;5;241m.\u001b[39mGROUP_BEAM_SEARCH:\n\u001b[0;32m   2075\u001b[0m     \u001b[38;5;66;03m# 11. prepare beam search scorer\u001b[39;00m\n\u001b[0;32m   2076\u001b[0m     beam_scorer \u001b[38;5;241m=\u001b[39m BeamSearchScorer(\n\u001b[0;32m   2077\u001b[0m         batch_size\u001b[38;5;241m=\u001b[39mbatch_size,\n\u001b[0;32m   2078\u001b[0m         num_beams\u001b[38;5;241m=\u001b[39mgeneration_config\u001b[38;5;241m.\u001b[39mnum_beams,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   2084\u001b[0m         max_length\u001b[38;5;241m=\u001b[39mgeneration_config\u001b[38;5;241m.\u001b[39mmax_length,\n\u001b[0;32m   2085\u001b[0m     )\n",
            "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\transformers\\generation\\utils.py:3279\u001b[0m, in \u001b[0;36mGenerationMixin._beam_search\u001b[1;34m(self, input_ids, beam_scorer, logits_processor, stopping_criteria, generation_config, synced_gpus, logits_warper, **model_kwargs)\u001b[0m\n\u001b[0;32m   3277\u001b[0m \u001b[38;5;66;03m# reshape for beam search\u001b[39;00m\n\u001b[0;32m   3278\u001b[0m vocab_size \u001b[38;5;241m=\u001b[39m next_token_scores\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\n\u001b[1;32m-> 3279\u001b[0m next_token_scores \u001b[38;5;241m=\u001b[39m \u001b[43mnext_token_scores\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mview\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_beams\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mvocab_size\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   3281\u001b[0m \u001b[38;5;66;03m# Beam token selection: pick 1 + eos_token_id.shape[0] next tokens for each beam so we have at least 1\u001b[39;00m\n\u001b[0;32m   3282\u001b[0m \u001b[38;5;66;03m# non eos token per beam.\u001b[39;00m\n\u001b[0;32m   3283\u001b[0m n_eos_tokens \u001b[38;5;241m=\u001b[39m eos_token_id\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;28;01mif\u001b[39;00m eos_token_id \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;241m0\u001b[39m\n",
            "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "# Okay this maybe works, maybe not, who knows, help me?\n",
        "input_ids = torch.tensor(test_x_data[\"input_ids\"]).to(model.device)\n",
        "print('translating the tokens..')\n",
        "translated_tokens = model.generate(input_ids=input_ids)\n",
        "predicted_translations = [tokenizer.decode(t, skip_special_tokens=True) for t in translated_tokens]\n",
        "\n",
        "references = [[ref] for ref in test_y_data] #The bleu score needs a list of lists\n",
        "bleu_score = bleu.compute(predictions=predicted_translations, references=references)\n",
        "\n",
        "print(f\"BLEU score: {bleu_score['score']}\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "llm_venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
